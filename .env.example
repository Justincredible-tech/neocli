# NeoCLI Configuration
# Copy this file to .env and customize as needed

# === OLLAMA CONFIGURATION ===
OLLAMA_HOST=http://127.0.0.1:11434

# === MODEL SELECTION ===
# Choose a model based on your hardware capabilities:
#   64GB+ RAM / RTX 4090:  qwen2.5-coder:32b
#   32GB RAM / RTX 3090:   qwen2.5-coder:14b
#   16GB RAM / RTX 3080:   qwen2.5-coder:7b
#   8GB RAM / M1 Mac:      qwen2.5-coder:3b
DEFAULT_MODEL=qwen2.5-coder:32b

# === CONTEXT WINDOW ===
# Adjust based on available memory (larger = more context but more memory)
CONTEXT_WINDOW_SIZE=32768

# === EMBEDDING MODEL ===
# Used for semantic memory (run: ollama pull nomic-embed-text)
EMBEDDING_MODEL=nomic-embed-text

# === SAFETY ===
# Set to 'false' to disable confirmation prompts for file writes (not recommended)
REQUIRE_APPROVAL=true

# === ADVANCED OPTIONS ===
# LLM_TIMEOUT_MS=300000
# MAX_AGENT_STEPS=30
